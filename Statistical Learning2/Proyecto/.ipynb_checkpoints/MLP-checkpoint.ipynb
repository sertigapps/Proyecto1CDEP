{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identificacion de lenguaje por medio de texto\n",
    "La identificacion de lenguaje es una de las caracteristicas mas comunes de todas las aplicaciones web y/o redes sociales, usualmente estan amarradas estrictamente a la traduccion para mejorar la experiencia y accesibilidad del contenido. El fin de esta tarea es lograr identificar el lenguaje natural de un texto. Lo cual tiene muchos casos de uso para post/tweets/articulos y mas.\n",
    "\n",
    "# El Dataset\n",
    "El dataset viene de  https://dumps.wikimedia.org para 7 lenguajes pero me en este caso me enfocare unicamente en Español, Ingles y Frances ya que son los unicos lenguajes que manejamos actualmente en mi ambito laboral\n",
    "# Proceso\n",
    "- PreProcesar el texto para realizar NLP\n",
    "- Construir y Experimentar el entramiento de un MLP usando Keras y Tensorflow\n",
    "- Evaluar el modelo\n",
    "- Realizar Pruebas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importemos los paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/tf/lib/python3.7/site-packages/sklearn/externals/joblib/__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "import keras.optimizers\n",
    "from sklearn.externals.joblib import dump, load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones de Ayuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "def definir_alfabeto():\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    spanish = 'áéíóúüñ'\n",
    "    all_lang_chars = base_en + french + spanish \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "\n",
    "\n",
    "def mostrar_matriz_de_conf(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    return fig\n",
    "\n",
    "def removertags(text):\n",
    "    return re.sub(r'<[^<]+?>', '', text)\n",
    "\n",
    "def removerparentesis(text):\n",
    "    return re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "\n",
    "def removerlineas(text):\n",
    "    return text.replace('\\n', ' ') \n",
    "    \n",
    "def removerespacios(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def limpiar_text(text):\n",
    "    text = removertags(text)\n",
    "    text = removerlineas(text)\n",
    "    text = removerlineas(text)\n",
    "    text = removerespacios(text)\n",
    "    return text\n",
    "\n",
    "#################\n",
    "# Pre Procesamiento #\n",
    "#################\n",
    "\n",
    "def obtener_sample(file_content,start_index,sample_size):\n",
    "    while not (file_content[start_index].isspace()):\n",
    "        start_index += 1\n",
    "    while file_content[start_index].isspace():\n",
    "        start_index += 1\n",
    "    end_index = start_index+sample_size \n",
    "    while not (file_content[end_index].isspace()):\n",
    "        end_index -= 1\n",
    "    return file_content[start_index:end_index]\n",
    "\n",
    "def contar_chars(text, alphabet):\n",
    "    alphabet_counts = []\n",
    "    for letter in alphabet:\n",
    "        count = text.count(letter)\n",
    "        alphabet_counts.append(count)\n",
    "    return alphabet_counts\n",
    "\n",
    "def obtener_fila(content,start_index,sample_size, alphabet):\n",
    "    sample_text = obtener_sample(content,start_index,sample_size)\n",
    "    counted_chars_all = contar_chars(sample_text.lower(), alphabet[0])\n",
    "    counted_chars_big = contar_chars(sample_text, alphabet[1])\n",
    "    all_parts = counted_chars_all + counted_chars_big\n",
    "    return all_parts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hiperparametros de entrenamiento\n",
    "Ahora vamos a definir los hiperparametros, en este caso sera el numero de iteraciones y el batch size ya que usaremos Batch Gradient Descent para nuestro entrenamiento, de igual forma si se corre en una pc con gpu disponible podemos aumentar el batch size para aumentar la tasa de convergencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si hay gpu disponible\n",
    "if tf.test.is_gpu_available():\n",
    "    # GPU\n",
    "    BATCH_SIZE = 512  # numero de records usados por epoch\n",
    "    EPOCHS = 12  # numero de epochs\n",
    "    \n",
    "# si no hay gpu disponible\n",
    "else:\n",
    "    # CPU\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparacion de el Dataset\n",
    "En este caso debido a que es un dataset bastante grande no estara subido en el repositorio github pero el data set viene de la siguiente URL https://www.floydhub.com/floydhub/datasets/language-identification/1 para entrenar el modelo se debera descargar y colocarlo en una carpeta local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alfabeto:\n",
      "abcdefghijklmnopqrstuvwxyzàáâæçèéêíîïñóôùúûüÿœ !?¿¡ABCDEFGHIJKLMNOPQRSTUVWXYZÀÁÂÆÇÈÉÊÍÎÏÑÓÔÙÚÛÜŒŸ\n",
      "Alfabeto len(TAM_VOCAB): 97\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# diccionario de lenguajes que nuestro calsificador considerara\n",
    "LENGUAJES_DICT = {'en':0,'fr':1,'es':2}\n",
    "\n",
    "# Largo maximo de caracters para entrenamiento y  prediccion \n",
    "LAR_MAX = 140\n",
    "\n",
    "# numero de muestras por lenguaje\n",
    "NUM_MUESTRAS = 250000\n",
    "\n",
    "# Seed para aleatoriedad\n",
    "SEED = 42\n",
    "\n",
    "alfabeto = definir_alfabeto()\n",
    "print('Alfabeto:')\n",
    "print(alfabeto[2])\n",
    "\n",
    "TAM_VOCAB = len(alfabeto[2])\n",
    "print('Alfabeto len(TAM_VOCAB):', TAM_VOCAB)\n",
    "\n",
    "# Folders from where load / store the raw, source, cleaned, samples and train_test data\n",
    "data_directory = \"/users/luisgarcia/floydhub-datasets-language-identification-1/data\"\n",
    "source_directory = os.path.join(data_directory, 'source')\n",
    "cleaned_directory = os.path.join(data_directory, 'cleaned')\n",
    "samples_directory = os.path.join('/tmp', 'samples')\n",
    "train_test_directory = os.path.join('/tmp', 'train_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. TEXTO MUESTRA: \n",
      " gibt es aber auch eine katholische, eine neuapostolische und eine baptistische Gemeinde sowie eine Gemeinde Gottes und eine Gemeinde der\n",
      "\n",
      "2. ALFABETO DE REFERENCIA: \n",
      " ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', 'á', 'â', 'æ', 'ç', 'è', 'é', 'ê', 'í', 'î', 'ï', 'ñ', 'ó', 'ô', 'ù', 'ú', 'û', 'ü', 'ÿ', 'œ', ' ', '!', '?', '¿', '¡', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'À', 'Á', 'Â', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Í', 'Î', 'Ï', 'Ñ', 'Ó', 'Ô', 'Ù', 'Ú', 'Û', 'Ü', 'Œ', 'Ÿ']\n",
      "\n",
      "3. FILA DE MUESTRA: \n",
      " [5, 3, 4, 6, 28, 0, 5, 5, 14, 0, 1, 2, 3, 11, 5, 2, 0, 2, 8, 7, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 19, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "4. TAMANO DE ENTRADA (VOCAB SIZE):  97\n"
     ]
    }
   ],
   "source": [
    "path = os.path.join(cleaned_directory, \"de_cleaned.txt\")\n",
    "with open(path, 'r') as f:\n",
    "    content = f.read()\n",
    "    random_index = random.randrange(0,len(content)-2*LAR_MAX)\n",
    "    sample_text = obtener_sample(content,random_index,LAR_MAX)\n",
    "    print (\"1. TEXTO MUESTRA: \\n\", sample_text)\n",
    "    print (\"\\n2. ALFABETO DE REFERENCIA: \\n\", alfabeto[0]+alfabeto[1])\n",
    "    \n",
    "    sample_input_row = obtener_fila(content, random_index, LAR_MAX, alfabeto)\n",
    "    print (\"\\n3. FILA DE MUESTRA: \\n\",sample_input_row)\n",
    "    \n",
    "    input_size = len(sample_input_row)\n",
    "    if input_size != TAM_VOCAB:\n",
    "        print(\"Algo raro paso!\")\n",
    "        \n",
    "    print (\"\\n4. TAMANO DE ENTRADA (VOCAB SIZE): \", input_size)\n",
    "    del content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando archivo : /users/luisgarcia/floydhub-datasets-language-identification-1/data/cleaned/en_cleaned.txt\n",
      "Tamano Archivo :  101.42 MB  | # Posibles muestras :  1045576 | # Caracteres Skipped : 199\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Procesando archivo : /users/luisgarcia/floydhub-datasets-language-identification-1/data/cleaned/fr_cleaned.txt\n",
      "Tamano Archivo :  98.72 MB  | # Posibles muestras :  1017760 | # Caracteres Skipped : 191\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Procesando archivo : /users/luisgarcia/floydhub-datasets-language-identification-1/data/cleaned/es_cleaned.txt\n",
      "Tamano Archivo :  97.56 MB  | # Posibles muestras :  1005801 | # Caracteres Skipped : 187\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Tamano Vocab :  97\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Numero de Muestras :  (750000, 98)\n",
      "/tmp/samples/lang_samples_97.npz Tamano :  21.60 MB\n"
     ]
    }
   ],
   "source": [
    "def size_en_mb(size):\n",
    "    size_mb =  '{:.2f}'.format(size/(1000*1000.0))\n",
    "    return size_mb + \" MB\"\n",
    "\n",
    "# Ahora que ya tenemos nuestras funciones de preprocesamiento , las vamos a usar para procesar los archivos de lenguajes\n",
    "# y convertir el texto en data numerica para nuestra red neuronal\n",
    "# prepara el array de numpy\n",
    "sample_data = np.empty((NUM_MUESTRAS*len(LENGUAJES_DICT),input_size+1),dtype = np.uint16)\n",
    "leng_seq = 0 # offset para cada lenguahes\n",
    "jump_reduce = 0.2 #variable para evitar pasarnos del final del archivo\n",
    "\n",
    "for leng in LENGUAJES_DICT:\n",
    "    start_index = 0\n",
    "    path = os.path.join(cleaned_directory, leng+\"_cleaned.txt\")\n",
    "    with open(path, 'r') as f:\n",
    "        print (\"Procesando archivo : \" + path)\n",
    "        file_content = f.read()\n",
    "        content_length = len(file_content)\n",
    "        remaining = content_length - LAR_MAX*NUM_MUESTRAS\n",
    "        jump = int(((remaining/NUM_MUESTRAS)*3)/4)\n",
    "        print (\"Tamano Archivo : \",size_en_mb(content_length),\\\n",
    "               \" | # Posibles muestras : \",int(content_length/TAM_VOCAB),\\\n",
    "              \"| # Caracteres Skipped : \" + str(jump))\n",
    "        for idx in range(NUM_MUESTRAS):\n",
    "            input_row = obtener_fila(file_content, start_index, LAR_MAX, alfabeto)\n",
    "            sample_data[NUM_MUESTRAS*leng_seq+idx,] = input_row + [LENGUAJES_DICT[leng]]\n",
    "            start_index += LAR_MAX + jump\n",
    "        del file_content\n",
    "    leng_seq += 1\n",
    "    print (100*\"-\")\n",
    "     \n",
    "# vamos a hacer shuffle a la data\n",
    "np.random.shuffle(sample_data)\n",
    "# reference input size\n",
    "print (\"Tamano Vocab : \",TAM_VOCAB )\n",
    "print (100*\"-\")\n",
    "print (\"Numero de Muestras : \",sample_data.shape )\n",
    "\n",
    "# crear el directiorio si no existe\n",
    "if not os.path.exists(samples_directory):\n",
    "    os.makedirs(samples_directory)\n",
    "\n",
    "# guardar la informacion comprimida en disco\n",
    "path_smpl = os.path.join(samples_directory,\"lang_samples_\"+str(TAM_VOCAB)+\".npz\")\n",
    "np.savez_compressed(path_smpl,data=sample_data)\n",
    "print(path_smpl, \"Tamano : \", size_en_mb(os.path.getsize(path_smpl)))\n",
    "del sample_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verificando que este bien todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra Aleatoria : \n",
      " [12  1  7  2 11  3  4  6 12  0  0  5  2  6  3  1  0  8 11  8  4  0  2  0\n",
      "  1  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0 20  0\n",
      "  0  0  0  2  0  1  0  0  0  0  1  0  0  0  1  0  0  0  0  0  0  0  0  1\n",
      "  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0\n",
      "  0  0]\n",
      "\n",
      "Lenguaje de Muestra :  en\n",
      "\n",
      "Shape Datase (Muestras, Alfabeto): (750000, 98)\n",
      "Conteo de muestras por lenguaje \n",
      "en 250000\n",
      "fr 250000\n",
      "es 250000\n"
     ]
    }
   ],
   "source": [
    "# obtener codigo lenguaje\n",
    "def get_lengid(lengid):    \n",
    "    for dname, did in LENGUAJES_DICT.items():\n",
    "        if did == lengid:\n",
    "            return dname\n",
    "\n",
    "# Cargando la informacion\n",
    "path_smpl = os.path.join(samples_directory,\"lang_samples_\"+str(TAM_VOCAB)+\".npz\")\n",
    "dt = np.load(path_smpl)['data']\n",
    "\n",
    "# Revisando una muestra aleatoria\n",
    "random_index = random.randrange(0,dt.shape[0])\n",
    "print (\"Muestra Aleatoria : \\n\",dt[random_index,])\n",
    "print (\"\\nLenguaje de Muestra : \",get_lengid(dt[random_index,][TAM_VOCAB]))\n",
    "\n",
    "# Check if the data have equal share of different languages\n",
    "print (\"\\nShape Datase (Muestras, Alfabeto):\", dt.shape)\n",
    "bins = np.bincount(dt[:,input_size])\n",
    "\n",
    "print (\"Conteo de muestras por lenguaje \") \n",
    "for lang_code in LENGUAJES_DICT: \n",
    "    print (lang_code, bins[LENGUAJES_DICT[lang_code]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento de datos\n",
    "Aunque la informacion ya esta lista para el entrenamiento tenemos que normalizarla esto para ayudar a nuestros modelos a alcanzar una taza de convergencia mas rapido ya que los datos son mas sencillos computacionalmente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muestra antes de preprocesamiento:\n",
      "X : \n",
      " [ 7.  1.  6.  5. 19.  1.  0.  2.  6.  0.  0.  4.  2. 10.  6.  4.  2.  7.\n",
      "  9.  8.  6.  1.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  3.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0. 23.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n",
      "Y : \n",
      " 1.0\n",
      "X pre procesado : (750000, 97)\n",
      "\n",
      "Muestra despues de preprocesamiento:\n",
      "X : \n",
      " [-8.93468618e-01 -3.32060695e-01  9.17920530e-01  2.44210977e-02\n",
      "  1.36987865e+00 -3.77986491e-01 -1.16430354e+00 -1.41217262e-01\n",
      " -5.96088231e-01 -5.38960636e-01 -4.69209522e-01 -7.20314026e-01\n",
      " -5.31015158e-01  7.82021105e-01 -5.18950820e-01  7.38213062e-01\n",
      "  1.76657856e+00 -9.19680148e-02  4.28143293e-01  2.28501022e-01\n",
      "  6.84853017e-01 -1.61697552e-01 -5.66822171e-01 -4.90597010e-01\n",
      " -8.87127697e-01 -4.17503774e-01 -3.53756905e-01 -3.18747878e-01\n",
      " -1.24981105e-01 -1.85475890e-02  6.24726534e+00 -3.27432334e-01\n",
      "  1.28216100e+00 -1.90451518e-01 -3.51090640e-01 -1.16210237e-01\n",
      " -6.56264871e-02 -2.38961145e-01 -4.23627198e-01 -1.24268427e-01\n",
      " -9.27703455e-02 -1.97303474e-01 -6.99068531e-02 -5.98397404e-02\n",
      " -7.91919325e-03 -8.14012587e-02  5.38813770e-01 -3.40077020e-02\n",
      " -4.12922464e-02 -1.82343721e-02 -1.47595592e-02 -5.10925472e-01\n",
      " -4.03479964e-01 -5.36267698e-01 -3.87232721e-01 -4.75243241e-01\n",
      " -3.51493716e-01 -3.38512719e-01 -3.44480097e-01 -3.80917490e-01\n",
      " -2.94480890e-01 -2.14806482e-01 -5.30845940e-01 -4.50821340e-01\n",
      " -3.21132839e-01 -2.77292073e-01 -4.36430484e-01 -1.06788620e-01\n",
      " -3.59243691e-01  8.76013577e-01 -4.59275991e-01 -2.58700073e-01\n",
      " -2.66542345e-01 -2.48516157e-01 -1.07511692e-01 -1.45644009e-01\n",
      " -1.10097803e-01 -8.71933475e-02 -5.92512153e-02 -2.71804724e-02\n",
      " -8.51080567e-03 -9.28516686e-03 -7.66689191e-03 -1.17394581e-01\n",
      " -4.09881491e-03 -1.57965850e-02 -3.01852711e-02 -2.18218402e-03\n",
      " -9.39051062e-03 -2.14915369e-02 -4.20087716e-03 -1.15470123e-03\n",
      " -1.86558291e-02  0.00000000e+00 -9.64573026e-03 -9.32090823e-03\n",
      "  0.00000000e+00]\n",
      "Y : \n",
      " [0. 1. 0.]\n",
      "/tmp/train_test/train_test_data_97.npz Tamano :  32.26 MB\n"
     ]
    }
   ],
   "source": [
    "# Aun nos falta un poco de preprocesamiento para poder entrenar nuestro MLP sobre todo en la escala\n",
    "# Escalar nos ayudara que el algoritmo optimizador va a converger de forma correcta\n",
    "# tambien tenemos que hacer one-hot encoding a nuestras clases para la salida de nuestro softmax\n",
    "# vamos a convertir todo los datos a float\n",
    "dt = dt.astype(np.float32)\n",
    "# X y Y Split\n",
    "X = dt[:, 0:input_size] # Muestras\n",
    "Y = dt[:, input_size] # El ultimo elemento es el label\n",
    "del dt\n",
    "\n",
    "# Index random para obtener la muestra\n",
    "random_index = random.randrange(0,X.shape[0])\n",
    "print(\"Muestra antes de preprocesamiento:\")\n",
    "print(\"X : \\n\", X[random_index,])\n",
    "print(\"Y : \\n\", Y[random_index])\n",
    "\n",
    "# X Pre Procesamiento\n",
    "# Normalizar y escalar\n",
    "standard_scaler = preprocessing.StandardScaler().fit(X)\n",
    "X = standard_scaler.transform(X)   \n",
    "print (\"X pre procesado :\", X.shape)\n",
    "\n",
    "# Y Pre Procesamiento\n",
    "# One-hot encoding\n",
    "Y = keras.utils.to_categorical(Y, num_classes=len(LENGUAJES_DICT))\n",
    "\n",
    "# See the sample data\n",
    "print(\"\\nMuestra despues de preprocesamiento:\")\n",
    "print(\"X : \\n\", X[random_index,])\n",
    "print(\"Y : \\n\", Y[random_index])\n",
    "\n",
    "# Train/test split. Vamos a usar static ya que necesitamos resultados comparables para diferentes corridas con\n",
    "# el fin de pruebas y toma de desiciones\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.20, random_state=SEED)\n",
    "del X, Y\n",
    "\n",
    "# Crear los directorios\n",
    "if not os.path.exists(train_test_directory):\n",
    "    os.makedirs(train_test_directory)\n",
    "\n",
    "# Guardar a disco\n",
    "path_tt = os.path.join(train_test_directory,\"train_test_data_\"+str(TAM_VOCAB)+\".npz\")\n",
    "np.savez_compressed(path_tt,X_train=X_train,Y_train=Y_train,X_test=X_test,Y_test=Y_test)\n",
    "print(path_tt, \"Tamano : \",size_en_mb(os.path.getsize(path_tt)))\n",
    "del X_train,Y_train,X_test,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRAIN - TEST SPLIT\n",
    "Vamos a separa un 80% para entrenamiento y un 20% para pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train:  (600000, 97)\n",
      "Y_train:  (600000, 3)\n",
      "X_test:  (150000, 97)\n",
      "Y_test:  (150000, 3)\n"
     ]
    }
   ],
   "source": [
    "path_tt = os.path.join(train_test_directory, \"train_test_data_\"+str(TAM_VOCAB)+\".npz\")\n",
    "train_test_data = np.load(path_tt)\n",
    "\n",
    "# Train Set\n",
    "X_train = train_test_data['X_train']\n",
    "print (\"X_train: \",X_train.shape)\n",
    "Y_train = train_test_data['Y_train']\n",
    "print (\"Y_train: \",Y_train.shape)\n",
    "\n",
    "# Test Set\n",
    "X_test = train_test_data['X_test']\n",
    "print (\"X_test: \",X_test.shape)\n",
    "Y_test = train_test_data['Y_test']\n",
    "print (\"Y_test: \",Y_test.shape)\n",
    "\n",
    "del train_test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arquitectura de nuestro MLP\n",
    "Ahora empezaremos a experimentar con nuestra arquitectura con multiples capaz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_8 (Dense)              (None, 100)               9800      \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 10,103\n",
      "Trainable params: 10,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 540000 samples, validate on 60000 samples\n",
      "Epoch 1/12\n",
      " - 10s - loss: 0.0363 - accuracy: 0.9882 - val_loss: 0.0303 - val_accuracy: 0.9898\n",
      "Epoch 2/12\n",
      " - 10s - loss: 0.0291 - accuracy: 0.9904 - val_loss: 0.0290 - val_accuracy: 0.9902\n",
      "Epoch 3/12\n",
      " - 10s - loss: 0.0274 - accuracy: 0.9909 - val_loss: 0.0277 - val_accuracy: 0.9906\n",
      "Epoch 4/12\n",
      " - 10s - loss: 0.0263 - accuracy: 0.9913 - val_loss: 0.0283 - val_accuracy: 0.9905\n",
      "Epoch 5/12\n",
      " - 10s - loss: 0.0258 - accuracy: 0.9915 - val_loss: 0.0287 - val_accuracy: 0.9904\n",
      "Epoch 6/12\n",
      " - 10s - loss: 0.0251 - accuracy: 0.9918 - val_loss: 0.0278 - val_accuracy: 0.9906\n",
      "Epoch 7/12\n",
      " - 10s - loss: 0.0249 - accuracy: 0.9918 - val_loss: 0.0276 - val_accuracy: 0.9907\n",
      "Epoch 8/12\n",
      " - 10s - loss: 0.0245 - accuracy: 0.9919 - val_loss: 0.0292 - val_accuracy: 0.9904\n",
      "Epoch 9/12\n",
      " - 10s - loss: 0.0241 - accuracy: 0.9921 - val_loss: 0.0288 - val_accuracy: 0.9904\n",
      "Epoch 10/12\n",
      " - 11s - loss: 0.0238 - accuracy: 0.9922 - val_loss: 0.0289 - val_accuracy: 0.9905\n",
      "Epoch 11/12\n",
      " - 11s - loss: 0.0235 - accuracy: 0.9923 - val_loss: 0.0297 - val_accuracy: 0.9902\n",
      "Epoch 12/12\n",
      " - 11s - loss: 0.0234 - accuracy: 0.9924 - val_loss: 0.0296 - val_accuracy: 0.9900\n",
      "150000/150000 [==============================] - 2s 16us/step\n",
      "accuracy: 99.06%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dense(len(LENGUAJES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "checkpoint = ModelCheckpoint('/users/luisgarcia/cpmlp', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=[checkpoint,tensorboard],\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 500)               49000     \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 3)                 1503      \n",
      "=================================================================\n",
      "Total params: 50,503\n",
      "Trainable params: 50,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 540000 samples, validate on 60000 samples\n",
      "Epoch 1/12\n",
      " - 13s - loss: 0.0562 - accuracy: 0.9830 - val_loss: 0.0644 - val_accuracy: 0.9843\n",
      "Epoch 2/12\n",
      " - 12s - loss: 0.0456 - accuracy: 0.9857 - val_loss: 0.0754 - val_accuracy: 0.9851\n",
      "Epoch 3/12\n",
      " - 12s - loss: 0.0435 - accuracy: 0.9864 - val_loss: 0.1040 - val_accuracy: 0.9847\n",
      "Epoch 4/12\n",
      " - 12s - loss: 0.0425 - accuracy: 0.9869 - val_loss: 0.0596 - val_accuracy: 0.9857\n",
      "Epoch 5/12\n",
      " - 12s - loss: 0.0403 - accuracy: 0.9875 - val_loss: 0.0672 - val_accuracy: 0.9846\n",
      "Epoch 6/12\n",
      " - 12s - loss: 0.0385 - accuracy: 0.9878 - val_loss: 0.1503 - val_accuracy: 0.9858\n",
      "Epoch 7/12\n",
      " - 12s - loss: 0.0387 - accuracy: 0.9882 - val_loss: 0.1691 - val_accuracy: 0.9851\n",
      "Epoch 8/12\n",
      " - 12s - loss: 0.0365 - accuracy: 0.9888 - val_loss: 0.0714 - val_accuracy: 0.9857\n",
      "Epoch 9/12\n",
      " - 12s - loss: 0.0363 - accuracy: 0.9890 - val_loss: 0.1688 - val_accuracy: 0.9854\n",
      "Epoch 10/12\n",
      " - 12s - loss: 0.0356 - accuracy: 0.9894 - val_loss: 0.2434 - val_accuracy: 0.9859\n",
      "Epoch 11/12\n",
      " - 12s - loss: 0.0368 - accuracy: 0.9896 - val_loss: 0.4799 - val_accuracy: 0.9855\n",
      "Epoch 12/12\n",
      " - 13s - loss: 0.0330 - accuracy: 0.9901 - val_loss: 0.5132 - val_accuracy: 0.9857\n",
      "150000/150000 [==============================] - 2s 15us/step\n",
      "accuracy: 98.60%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dense(len(LENGUAJES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "checkpoint = ModelCheckpoint('/users/luisgarcia/cpmlp', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=[checkpoint,tensorboard],\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_16 (Dense)             (None, 500)               49000     \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 229,703\n",
      "Trainable params: 229,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 540000 samples, validate on 60000 samples\n",
      "Epoch 1/12\n",
      " - 25s - loss: 0.0547 - accuracy: 0.9828 - val_loss: 0.0475 - val_accuracy: 0.9848\n",
      "Epoch 2/12\n",
      " - 26s - loss: 0.0443 - accuracy: 0.9855 - val_loss: 0.0437 - val_accuracy: 0.9854\n",
      "Epoch 3/12\n",
      " - 27s - loss: 0.0413 - accuracy: 0.9865 - val_loss: 0.0467 - val_accuracy: 0.9861\n",
      "Epoch 4/12\n",
      " - 27s - loss: 0.0387 - accuracy: 0.9871 - val_loss: 0.0438 - val_accuracy: 0.9865\n",
      "Epoch 5/12\n",
      " - 27s - loss: 0.0369 - accuracy: 0.9877 - val_loss: 0.0437 - val_accuracy: 0.9857\n",
      "Epoch 6/12\n",
      " - 27s - loss: 0.0349 - accuracy: 0.9883 - val_loss: 0.0454 - val_accuracy: 0.9856\n",
      "Epoch 7/12\n",
      " - 28s - loss: 0.0328 - accuracy: 0.9890 - val_loss: 0.0502 - val_accuracy: 0.9855\n",
      "Epoch 8/12\n",
      " - 27s - loss: 0.0316 - accuracy: 0.9895 - val_loss: 0.0471 - val_accuracy: 0.9851\n",
      "Epoch 9/12\n",
      " - 27s - loss: 0.0290 - accuracy: 0.9902 - val_loss: 0.0480 - val_accuracy: 0.9855\n",
      "Epoch 10/12\n",
      " - 28s - loss: 0.0274 - accuracy: 0.9904 - val_loss: 0.0535 - val_accuracy: 0.9849\n",
      "Epoch 11/12\n",
      " - 28s - loss: 0.0256 - accuracy: 0.9911 - val_loss: 0.0546 - val_accuracy: 0.9848\n",
      "Epoch 12/12\n",
      " - 28s - loss: 0.0237 - accuracy: 0.9916 - val_loss: 0.0739 - val_accuracy: 0.9852\n",
      "150000/150000 [==============================] - 4s 27us/step\n",
      "accuracy: 98.58%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dense(len(LENGUAJES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "checkpoint = ModelCheckpoint('/users/luisgarcia/cpmlp', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=[checkpoint,tensorboard],\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_10 (Dense)             (None, 500)               49000     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 300)               150300    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 300)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 100)               30100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 229,703\n",
      "Trainable params: 229,703\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 540000 samples, validate on 60000 samples\n",
      "Epoch 1/12\n",
      " - 30s - loss: 0.0739 - accuracy: 0.9785 - val_loss: 0.0514 - val_accuracy: 0.9841\n",
      "Epoch 2/12\n",
      " - 29s - loss: 0.0606 - accuracy: 0.9828 - val_loss: 0.0490 - val_accuracy: 0.9845\n",
      "Epoch 3/12\n",
      " - 29s - loss: 0.0593 - accuracy: 0.9832 - val_loss: 0.0537 - val_accuracy: 0.9841\n",
      "Epoch 4/12\n",
      " - 29s - loss: 0.0589 - accuracy: 0.9834 - val_loss: 0.0506 - val_accuracy: 0.9846\n",
      "Epoch 5/12\n",
      " - 29s - loss: 0.0581 - accuracy: 0.9837 - val_loss: 0.0502 - val_accuracy: 0.9849\n",
      "Epoch 6/12\n",
      " - 30s - loss: 0.0574 - accuracy: 0.9839 - val_loss: 0.0571 - val_accuracy: 0.9842\n",
      "Epoch 7/12\n",
      " - 30s - loss: 0.0585 - accuracy: 0.9837 - val_loss: 0.0512 - val_accuracy: 0.9854\n",
      "Epoch 8/12\n",
      " - 31s - loss: 0.0589 - accuracy: 0.9839 - val_loss: 0.0497 - val_accuracy: 0.9841\n",
      "Epoch 9/12\n",
      " - 31s - loss: 0.0592 - accuracy: 0.9836 - val_loss: 0.0505 - val_accuracy: 0.9846\n",
      "Epoch 10/12\n",
      " - 31s - loss: 0.0598 - accuracy: 0.9839 - val_loss: 0.0618 - val_accuracy: 0.9850\n",
      "Epoch 11/12\n",
      " - 31s - loss: 0.0583 - accuracy: 0.9841 - val_loss: 0.0545 - val_accuracy: 0.9856\n",
      "Epoch 12/12\n",
      " - 31s - loss: 0.0579 - accuracy: 0.9838 - val_loss: 0.0558 - val_accuracy: 0.9858\n",
      "150000/150000 [==============================] - 4s 25us/step\n",
      "accuracy: 98.58%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(500,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(300, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(LENGUAJES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "checkpoint = ModelCheckpoint('/users/luisgarcia/cpmlp', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=[checkpoint,tensorboard],\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_17 (Dense)             (None, 100)               9800      \n",
      "_________________________________________________________________\n",
      "dropout_13 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_15 (Dropout)         (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 3)                 153       \n",
      "=================================================================\n",
      "Total params: 25,103\n",
      "Trainable params: 25,103\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 540000 samples, validate on 60000 samples\n",
      "Epoch 1/12\n",
      " - 15s - loss: 0.1029 - accuracy: 0.9668 - val_loss: 0.0482 - val_accuracy: 0.9845\n",
      "Epoch 2/12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/tf/lib/python3.7/site-packages/keras/callbacks/callbacks.py:707: RuntimeWarning: Can save best model only with val_acc available, skipping.\n",
      "  'skipping.' % (self.monitor), RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - 15s - loss: 0.0606 - accuracy: 0.9821 - val_loss: 0.0475 - val_accuracy: 0.9847\n",
      "Epoch 3/12\n",
      " - 14s - loss: 0.0573 - accuracy: 0.9828 - val_loss: 0.0459 - val_accuracy: 0.9853\n",
      "Epoch 4/12\n",
      " - 14s - loss: 0.0552 - accuracy: 0.9832 - val_loss: 0.0459 - val_accuracy: 0.9855\n",
      "Epoch 5/12\n",
      " - 14s - loss: 0.0545 - accuracy: 0.9834 - val_loss: 0.0450 - val_accuracy: 0.9854\n",
      "Epoch 6/12\n",
      " - 14s - loss: 0.0529 - accuracy: 0.9839 - val_loss: 0.0447 - val_accuracy: 0.9859\n",
      "Epoch 7/12\n",
      " - 14s - loss: 0.0523 - accuracy: 0.9840 - val_loss: 0.0434 - val_accuracy: 0.9861\n",
      "Epoch 8/12\n",
      " - 15s - loss: 0.0517 - accuracy: 0.9842 - val_loss: 0.0438 - val_accuracy: 0.9862\n",
      "Epoch 9/12\n",
      " - 15s - loss: 0.0512 - accuracy: 0.9843 - val_loss: 0.0435 - val_accuracy: 0.9864\n",
      "Epoch 10/12\n",
      " - 15s - loss: 0.0511 - accuracy: 0.9844 - val_loss: 0.0437 - val_accuracy: 0.9864\n",
      "Epoch 11/12\n",
      " - 15s - loss: 0.0508 - accuracy: 0.9846 - val_loss: 0.0429 - val_accuracy: 0.9864\n",
      "Epoch 12/12\n",
      " - 15s - loss: 0.0502 - accuracy: 0.9847 - val_loss: 0.0438 - val_accuracy: 0.9864\n",
      "150000/150000 [==============================] - 3s 17us/step\n",
      "accuracy: 98.63%\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(100,input_dim=input_size, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(100, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(50, kernel_initializer=\"glorot_uniform\", activation=\"sigmoid\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(len(LENGUAJES_DICT), kernel_initializer=\"glorot_uniform\", activation=\"softmax\"))\n",
    "model_optimizer = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=model_optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "checkpoint = ModelCheckpoint('/users/luisgarcia/cpmlp', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "history = model.fit(X_train,Y_train,\n",
    "                  epochs=EPOCHS,\n",
    "                  validation_split=0.1,\n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  callbacks=[checkpoint,tensorboard],\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELO ELEGIDO\n",
    "\n",
    "- Capas Ocultas 3\n",
    "- Dropout en cada Capa Oculta para evitar overfitting\n",
    "- Optimizador Adam\n",
    "- Activacion Sigmoid en las capaz intermedias\n",
    "- Activacion softmax en la capa de salida\n",
    "- Funcion de error : categorical_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_pred = model.predict_classes(X_test)\n",
    "Y_pred = keras.utils.to_categorical(Y_pred, num_classes=len(LENGUAJES_DICT))\n",
    "LABELS =  list(LENGUAJES_DICT.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cnf_matrix = confusion_matrix(np.argmax(Y_pred,axis=1), np.argmax(Y_test,axis=1))\n",
    "_ = mostrar_matriz_de_conf(cnf_matrix, LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          en       0.98      0.99      0.99     49913\n",
      "          fr       0.99      0.99      0.99     50080\n",
      "          es       0.99      0.98      0.98     50007\n",
      "\n",
      "   micro avg       0.99      0.99      0.99    150000\n",
      "   macro avg       0.99      0.99      0.99    150000\n",
      "weighted avg       0.99      0.99      0.99    150000\n",
      " samples avg       0.99      0.99      0.99    150000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(Y_test, Y_pred, target_names=LABELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pruebas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecir(TEXT):\n",
    "    cleaned_text = limpiar_text(TEXT)\n",
    "    print(cleaned_text)\n",
    "    print(len(cleaned_text))\n",
    "    input_row = obtener_fila(cleaned_text, 0, LAR_MAX, alfabeto)\n",
    "    print(input_row)\n",
    "    test_array = standard_scaler.transform([input_row])\n",
    "    \n",
    "    raw_score = model.predict(test_array)\n",
    "    \n",
    "    print(raw_score)\n",
    "    pred_idx= np.argmax(raw_score, axis=1)[0]\n",
    "    score = raw_score[0][pred_idx]*100\n",
    "    print('PREDICIENDO')\n",
    "    prediction = LABELS[model.predict_classes(test_array)[0]]\n",
    "    print('TEXTO:', TEXT, '\\nPREDICCION:', prediction.upper(), '\\nSCORE:', score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dedicada esta sección a la reseña de los compositores nativos y obras que han producido, con ligeros comentarios propios a cada uno, parécenos oportuno dar ligeras noticias sobre el origen de la composición.\n",
      "207\n",
      "[12, 1, 8, 4, 10, 0, 1, 1, 7, 0, 0, 3, 2, 7, 16, 5, 1, 8, 12, 4, 3, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[1.5282501e-06 1.0179316e-05 9.9998832e-01]]\n",
      "PREDICIENDO\n",
      "TEXTO: Dedicada esta sección a la reseña de los compositores nativos y obras que han producido, con ligeros comentarios propios a cada uno, parécenos oportuno dar ligeras noticias sobre el origen de la composición. \n",
      "PREDICCION: ES \n",
      "SCORE: 99.9988317489624\n"
     ]
    }
   ],
   "source": [
    "predecir(\"Dedicada esta sección a la reseña de los compositores nativos y obras que han producido, con ligeros comentarios propios a cada uno, parécenos oportuno dar ligeras noticias sobre el origen de la composición.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The HackerRank Virtual Career Fair has three days packed with speakers, panels, and recruiter AMAs. We are kicking off Monday with a keynote from Gayle McDowell, the author of Cracking the Coding Interview!\n",
      "206\n",
      "[16, 0, 5, 4, 15, 3, 1, 5, 7, 0, 7, 2, 2, 6, 3, 3, 0, 12, 6, 6, 2, 1, 3, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 21, 0, 0, 0, 0, 2, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[9.996673e-01 5.902279e-05 2.737781e-04]]\n",
      "PREDICIENDO\n",
      "TEXTO: The HackerRank Virtual Career Fair has three days packed with speakers, panels, and recruiter AMAs. We are kicking off Monday with a keynote from Gayle McDowell, the author of Cracking the Coding Interview! \n",
      "PREDICCION: EN \n",
      "SCORE: 99.96672868728638\n"
     ]
    }
   ],
   "source": [
    "predecir(\"The HackerRank Virtual Career Fair has three days packed with speakers, panels, and recruiter AMAs. We are kicking off Monday with a keynote from Gayle McDowell, the author of Cracking the Coding Interview!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voilà cinq mois que j'en faisais fonction, et, ma foi, je supportais bien cette responsabilité et goûtais fort cette indépendance. Je puis même affirmer, sans me flatter.\n",
      "170\n",
      "[7, 2, 5, 2, 15, 4, 1, 0, 13, 3, 0, 1, 4, 9, 8, 5, 2, 3, 9, 11, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 21, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[5.0314009e-07 9.9997270e-01 2.6766382e-05]]\n",
      "PREDICIENDO\n",
      "TEXTO: Voilà cinq mois que j'en faisais fonction, et, ma foi, je supportais bien cette responsabilité et goûtais fort cette indépendance. Je puis même affirmer, sans me flatter. \n",
      "PREDICCION: FR \n",
      "SCORE: 99.99727010726929\n"
     ]
    }
   ],
   "source": [
    "predecir(\"Voilà cinq mois que j'en faisais fonction, et, ma foi, je supportais bien cette responsabilité et goûtais fort cette indépendance. Je puis même affirmer, sans me flatter.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hola como estas mi nombre es luis renato garcia lopez y estoy estudiando en la universidad galileo una maestria en ciencia de datos , la carrera es muy interesante\n",
      "163\n",
      "[16, 1, 5, 6, 15, 0, 2, 0, 10, 0, 0, 6, 4, 8, 9, 1, 0, 8, 9, 6, 4, 1, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 24, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[3.4452189e-06 2.0917732e-05 9.9997568e-01]]\n",
      "PREDICIENDO\n",
      "TEXTO: Hola como estas mi nombre es luis renato garcia lopez y estoy estudiando en la universidad galileo una maestria en ciencia de datos , la carrera es muy interesante \n",
      "PREDICCION: ES \n",
      "SCORE: 99.99756813049316\n"
     ]
    }
   ],
   "source": [
    "predecir(\"Hola como estas mi nombre es luis renato garcia lopez y estoy estudiando en la universidad galileo una maestria en ciencia de datos , la carrera es muy interesante\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Buses are on the way, they have left the school. Thank you for your flexibility today. Buses are on the way, they have left the school. Thank you for your flexibility today. \n",
      "175\n",
      "[8, 3, 2, 1, 15, 4, 0, 11, 3, 0, 1, 6, 0, 3, 10, 0, 0, 4, 6, 11, 4, 2, 2, 1, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 25, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[[9.9985468e-01 3.2273161e-05 1.1293934e-04]]\n",
      "PREDICIENDO\n",
      "TEXTO:  Buses are on the way, they have left the school.  Thank you for your flexibility today. Buses are on the way, they have left the school.  Thank you for your flexibility today.  \n",
      "PREDICCION: EN \n",
      "SCORE: 99.98546838760376\n"
     ]
    }
   ],
   "source": [
    "predecir(\" Buses are on the way, they have left the school.  Thank you for your flexibility today. Buses are on the way, they have left the school.  Thank you for your flexibility today. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vamos a Guardar el modelo ya que lo vamos a utilizar para la extraccion de datos de nuestra CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['std_scaler.bin']"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save('modeloMLP.h5')\n",
    "dump(standard_scaler, 'std_scaler.bin', compress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
