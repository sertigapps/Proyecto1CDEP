{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN para Named Entity Recognition de un custom tag\n",
    "En nuestro modelo anterior analizamos el audio de spots publicitarios en el cual utilizamos el script para categorizarlo por lenguaje, en este modelo vamos a analizar todos los Spots en los cuales el clientname aparece dentro del script para poder de esta forma ponderizar mas este token al momento de realizar busquedas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "from sklearn.externals.joblib import dump, load\n",
    "import keras\n",
    "import csv\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import sys\n",
    "csv.field_size_limit(sys.maxsize)\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "from tensorflow.keras import Sequential, Model, Input\n",
    "from tensorflow.keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional, Lambda\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tensorflow.random.set_seed(2)\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from keras.layers.merge import add\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "LENGUAJES_DICT = {'en':0,'fr':1,'es':2}\n",
    "modeloMLP = keras.models.load_model('modeloMLP.h5')\n",
    "sc=load('std_scaler.bin')\n",
    "LABELS =  list(LENGUAJES_DICT.keys())\n",
    "def removertags(text):\n",
    "    return re.sub(r'<[^<]+?>', '', text)\n",
    "\n",
    "def removerparentesis(text):\n",
    "    return re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", text)\n",
    "\n",
    "def removerlineas(text):\n",
    "    return text.replace('\\n', ' ') \n",
    "    \n",
    "def removerespacios(text):\n",
    "    return re.sub(r'\\s+', ' ', text)\n",
    "\n",
    "def limpiar_text(text):\n",
    "    text = removertags(text)\n",
    "    text = removerlineas(text)\n",
    "    text = removerlineas(text)\n",
    "    text = removerespacios(text)\n",
    "    return text\n",
    "def obtener_sample(file_content,start_index,sample_size):\n",
    "    while not (file_content[start_index].isspace()):\n",
    "        start_index += 1\n",
    "    while file_content[start_index].isspace():\n",
    "        start_index += 1\n",
    "    end_index = start_index+sample_size \n",
    "    while not (file_content[end_index].isspace()):\n",
    "        end_index -= 1\n",
    "    return file_content[start_index:end_index]\n",
    "def definir_alfabeto():\n",
    "    base_en = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    special_chars = ' !?¿¡'\n",
    "    french = 'àâæçéèêêîïôœùûüÿ'\n",
    "    spanish = 'áéíóúüñ'\n",
    "    all_lang_chars = base_en + french + spanish \n",
    "    small_chars = list(set(list(all_lang_chars)))\n",
    "    small_chars.sort() \n",
    "    big_chars = list(set(list(all_lang_chars.upper())))\n",
    "    big_chars.sort()\n",
    "    small_chars += special_chars\n",
    "    letters_string = ''\n",
    "    letters = small_chars + big_chars\n",
    "    for letter in letters:\n",
    "        letters_string += letter\n",
    "    return small_chars,big_chars,letters_string\n",
    "alfabeto = definir_alfabeto()\n",
    "def contar_chars(text, alphabet):\n",
    "    alphabet_counts = []\n",
    "    for letter in alphabet:\n",
    "        count = text.count(letter)\n",
    "        alphabet_counts.append(count)\n",
    "    return alphabet_counts\n",
    "\n",
    "def obtener_fila(content,start_index,sample_size, alphabet):\n",
    "    sample_text = obtener_sample(content,start_index,sample_size)\n",
    "    counted_chars_all = contar_chars(sample_text.lower(), alphabet[0])\n",
    "    counted_chars_big = contar_chars(sample_text, alphabet[1])\n",
    "    all_parts = counted_chars_all + counted_chars_big\n",
    "    return all_parts\n",
    "def predecir(TEXT):\n",
    "    cleaned_text = limpiar_text(TEXT)\n",
    "    while(len(cleaned_text) < 150 ):\n",
    "        cleaned_text += ' ' + cleaned_text\n",
    "    input_row = obtener_fila(cleaned_text, 0, 140, alfabeto)\n",
    "    test_array = sc.transform([input_row])\n",
    "    raw_score = modeloMLP.predict(test_array)\n",
    "    pred_idx= np.argmax(raw_score, axis=1)[0]\n",
    "    score = raw_score[0][pred_idx]*100\n",
    "    prediction = LABELS[modeloMLP.predict_classes(test_array)[0]]\n",
    "    return prediction\n",
    "def remove_html_tags(text):\n",
    "    \"\"\"Remove html tags from a string\"\"\"\n",
    "    import re\n",
    "    clean = re.compile('<.*?>')\n",
    "    text = re.sub(clean, ' ', text)\n",
    "    regex = re.compile(\"\\((.*?)\\)\")\n",
    "    text = re.sub(regex,'', text)\n",
    "    regex = re.compile(\"&.*?;\")\n",
    "    text = re.sub(regex,'', text)\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto voy a separar todos los scripts en sus oraciones por medio de , . \\<br> o nueva linea para que de esta forma cada elemento del dataframe contenga las supuestas oraciones incluidas en el script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 65139\n"
     ]
    }
   ],
   "source": [
    "crs = []\n",
    "i = 0\n",
    "with open('querySpotsClientInScript.csv', newline='') as File:  \n",
    "    reader = csv.reader(File)\n",
    "    for row in reader:\n",
    "        scriptnospaces = re.sub('[^A-Za-z0-9]+','', remove_html_tags(row[4]))\n",
    "        if( len(scriptnospaces) > 15 ):\n",
    "            lang = predecir(script)\n",
    "            if lang == 'fr':\n",
    "                lang = 'es' \n",
    "            if lang == 'es' or lang == 'en' or lang == 'fr':\n",
    "                sentences = row[4].split(',')\n",
    "                for s in sentences:\n",
    "                    actuals = s.split('.')\n",
    "                    for s2 in actuals:\n",
    "                        actuals2 = s2.split('\\n')\n",
    "                        for s3 in actuals2:\n",
    "                            actuals3 = s3.split('<br>')\n",
    "                            for s4 in actuals3:\n",
    "                                if len(s4.split(' ')) > 3:\n",
    "                                    crs.append([\n",
    "                                        row[1].lower(),\n",
    "                                        remove_html_tags(s4.lower()),\n",
    "                                        lang\n",
    "                                       ])\n",
    "            b = \"Loading \" + str(i)\n",
    "            print (b, end=\"\\r\")\n",
    "            i+=1\n",
    "                \n",
    "crdf = DataFrame(crs,columns=['clientname','script', lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientname</th>\n",
       "      <th>script</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>590415</td>\n",
       "      <td>590415</td>\n",
       "      <td>590415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>5392</td>\n",
       "      <td>260516</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>menards</td>\n",
       "      <td></td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>20057</td>\n",
       "      <td>1965</td>\n",
       "      <td>590415</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       clientname  script      es\n",
       "count      590415  590415  590415\n",
       "unique       5392  260516       1\n",
       "top       menards              es\n",
       "freq        20057    1965  590415"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crdf.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clientname</th>\n",
       "      <th>script</th>\n",
       "      <th>es</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>icon night club</td>\n",
       "      <td>icon night club the newest hottest spot in to...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>icon night club</td>\n",
       "      <td>meringue barchata y rock en espanol</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>icon night club</td>\n",
       "      <td>ladies get in for free before 11pm</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>icon night club</td>\n",
       "      <td>$ 1 dollar mix drinks before 11pm</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>icon night club</td>\n",
       "      <td>be our vip guest - call 214-351-1622 and rese...</td>\n",
       "      <td>es</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        clientname                                             script  es\n",
       "0  icon night club   icon night club the newest hottest spot in to...  es\n",
       "1  icon night club               meringue barchata y rock en espanol   es\n",
       "2  icon night club               ladies get in for free before 11pm    es\n",
       "3  icon night club                $ 1 dollar mix drinks before 11pm    es\n",
       "4  icon night club   be our vip guest - call 214-351-1622 and rese...  es"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crdf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' offers easy credit financing and has spanish speaking staff in every department? '"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crdf.iloc[46564]['script']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "590415"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocesamiento\n",
    "Ahora vamos a pasar nlp a cada una de las oraciones vamos a remover todos los spacios y vamos a marcar cada token con \n",
    "las siguientes caracteristicas\n",
    "Numero de Oracion, Token, POS, Tag\n",
    "\n",
    "El tag vamos a sacarlo por medio de dos opciones ya sea un O para entidad desconocida o C para la entidad cliente si el \n",
    "token esta incluido en el nombre de cliente lo vamos a marcar como C\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2304961"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "tokens = []\n",
    "for i in range(len(crdf)):\n",
    "    sentence = crdf.iloc[i]\n",
    "    if sentence['clientname'] in sentence['script']:\n",
    "        doc = nlp(sentence['script'])\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'SPACE':\n",
    "                tag = 'O'\n",
    "                if token.text in sentence['clientname']:\n",
    "                    tag = 'C'\n",
    "                tokens.append([i+1,token.text,token.pos_,tag])\n",
    "    print (i, end=\"\\r\")\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a sacar el identificador de nuestros tokens y un identificador del tag asociado al token esto para poder enviar valores numericos a nuestra red neuronal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "def get_dict(data, token_or_tag):\n",
    "    tok2idx = {}\n",
    "    idx2tok = {}\n",
    "    \n",
    "    if token_or_tag == 'token':\n",
    "        vocab = list(set(data['text'].to_list()))\n",
    "    else:\n",
    "        vocab = list(set(data['tag'].to_list()))\n",
    "    \n",
    "    idx2tok = {idx:tok for  idx, tok in enumerate(vocab)}\n",
    "    tok2idx = {tok:idx for  idx, tok in enumerate(vocab)}\n",
    "    return tok2idx, idx2tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "datatokens = DataFrame(tokens,columns=['sentence','text', 'post','tag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>post</th>\n",
       "      <th>tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>icon</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>club</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>newest</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence    text   post tag\n",
       "0         1    icon  PROPN   C\n",
       "1         1   night   NOUN   C\n",
       "2         1    club  PROPN   C\n",
       "3         1     the    DET   O\n",
       "4         1  newest    ADJ   O"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatokens.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "token2idx, idx2token = get_dict(datatokens, 'token')\n",
    "tag2idx, idx2tag = get_dict(datatokens, 'tag')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>post</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>icon</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>C</td>\n",
       "      <td>44421</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>night</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>C</td>\n",
       "      <td>34252</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>club</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>C</td>\n",
       "      <td>38693</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>the</td>\n",
       "      <td>DET</td>\n",
       "      <td>O</td>\n",
       "      <td>14732</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>newest</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>O</td>\n",
       "      <td>30137</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence    text   post tag  text_idx  tag_idx\n",
       "0         1    icon  PROPN   C     44421        0\n",
       "1         1   night   NOUN   C     34252        0\n",
       "2         1    club  PROPN   C     38693        0\n",
       "3         1     the    DET   O     14732        1\n",
       "4         1  newest    ADJ   O     30137        1"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatokens['text_idx'] = datatokens['text'].map(token2idx)\n",
    "datatokens['tag_idx'] = datatokens['tag'].map(tag2idx)\n",
    "datatokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a agrupar todas las oraciones con sus respectivos arreglos a marcar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:3: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>post</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[icon, night, club, the, newest, hottest, spot...</td>\n",
       "      <td>[PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...</td>\n",
       "      <td>[C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]</td>\n",
       "      <td>[44421, 34252, 38693, 14732, 30137, 18441, 171...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>[icon, night, club, your, new, favorite, club,...</td>\n",
       "      <td>[PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, NUM,...</td>\n",
       "      <td>[C, C, C, O, O, O, C, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[44421, 34252, 38693, 9895, 43491, 11425, 3869...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>[icon, night, club, el, sitio, mas, nuevo, y, ...</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, PRO...</td>\n",
       "      <td>[C, C, C, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[44421, 34252, 38693, 3136, 44105, 5247, 8180,...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15</td>\n",
       "      <td>[valet, parking, disponible, icon, night, club...</td>\n",
       "      <td>[PROPN, NOUN, ADJ, NOUN, NOUN, NOUN, PUNCT]</td>\n",
       "      <td>[O, O, O, C, C, C, O]</td>\n",
       "      <td>[31503, 5000, 40297, 44421, 34252, 38693, 9605]</td>\n",
       "      <td>[1, 1, 1, 0, 0, 0, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>18</td>\n",
       "      <td>[el, restaurante, que, esta, en, la, boca, de,...</td>\n",
       "      <td>[PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, PRO...</td>\n",
       "      <td>[O, O, O, O, O, C, O, O, O, O, C, C, C, O, O, O]</td>\n",
       "      <td>[3136, 39991, 9517, 21459, 35579, 32034, 22893...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence                                               text  \\\n",
       "0         1  [icon, night, club, the, newest, hottest, spot...   \n",
       "1         8  [icon, night, club, your, new, favorite, club,...   \n",
       "2         9  [icon, night, club, el, sitio, mas, nuevo, y, ...   \n",
       "3        15  [valet, parking, disponible, icon, night, club...   \n",
       "4        18  [el, restaurante, que, esta, en, la, boca, de,...   \n",
       "\n",
       "                                                post  \\\n",
       "0  [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...   \n",
       "1  [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, NUM,...   \n",
       "2  [PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, PRO...   \n",
       "3        [PROPN, NOUN, ADJ, NOUN, NOUN, NOUN, PUNCT]   \n",
       "4  [PROPN, PROPN, PROPN, PROPN, PROPN, PROPN, PRO...   \n",
       "\n",
       "                                                 tag  \\\n",
       "0      [C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]   \n",
       "1  [C, C, C, O, O, O, C, O, O, O, O, O, O, O, O, ...   \n",
       "2  [C, C, C, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "3                              [O, O, O, C, C, C, O]   \n",
       "4   [O, O, O, O, O, C, O, O, O, O, C, C, C, O, O, O]   \n",
       "\n",
       "                                            text_idx  \\\n",
       "0  [44421, 34252, 38693, 14732, 30137, 18441, 171...   \n",
       "1  [44421, 34252, 38693, 9895, 43491, 11425, 3869...   \n",
       "2  [44421, 34252, 38693, 3136, 44105, 5247, 8180,...   \n",
       "3    [31503, 5000, 40297, 44421, 34252, 38693, 9605]   \n",
       "4  [3136, 39991, 9517, 21459, 35579, 32034, 22893...   \n",
       "\n",
       "                                             tag_idx  \n",
       "0      [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "1  [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3                              [1, 1, 1, 0, 0, 0, 1]  \n",
       "4   [1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]  "
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatokensg = datatokens.groupby(\n",
    "['sentence'],as_index=False\n",
    ")['text', 'post', 'tag', 'text_idx', 'tag_idx'].agg(lambda x: list(x))\n",
    "datatokensg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['text'].to_list())))\n",
    "    n_tag = len(list(set(data['tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['text_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    \n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags\n",
    "\n",
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(datatokensg, datatokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(93585, 231)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31196, 231)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13865, 231)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "93585"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31196"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13865"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  44938 \n",
      "output_dim:  64 \n",
      "input_length:  231 \n",
      "n_tags:  2\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(datatokens['text'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in datatokensg['text_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_6 (Embedding)      (None, 231, 64)           2876032   \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 231, 128)          66048     \n",
      "_________________________________________________________________\n",
      "lstm_13 (LSTM)               (None, 231, 64)           49408     \n",
      "_________________________________________________________________\n",
      "time_distributed_6 (TimeDist (None, 231, 2)            130       \n",
      "=================================================================\n",
      "Total params: 2,991,618\n",
      "Trainable params: 2,991,618\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "2340/2340 - 1824s - loss: nan - accuracy: 0.0126 - val_loss: nan - val_accuracy: 0.0126\n",
      "Epoch 2/2\n",
      "2340/2340 - 1756s - loss: nan - accuracy: 0.0125 - val_loss: nan - val_accuracy: 0.0126\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_test' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-a4f0808d4a89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                   \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                   verbose=2)\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_test' is not defined"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "history = model.fit(train_tokens,np.array(train_tags),\n",
    "                  epochs=2,\n",
    "                  validation_split=0.20,\n",
    "                  shuffle=True,\n",
    "                  verbose=2)\n",
    "scores = model.evaluate(X_test, Y_test, verbose=1)\n",
    "print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este punto me di cuenta que algo estaba mal ya que el accuracy estuvo muy bajo y no hubo mejora en el modelo y esto fue debido a que mezcle los dos lenguajes de mis scripts mientras que el nlp a usar deberia de recibir solo scripts en ingles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 65138\r"
     ]
    }
   ],
   "source": [
    "crs = []\n",
    "i = 0\n",
    "with open('querySpotsClientInScript.csv', newline='') as File:  \n",
    "    reader = csv.reader(File)\n",
    "    for row in reader:\n",
    "        scriptnospaces = re.sub('[^A-Za-z0-9]+','', remove_html_tags(row[4]))\n",
    "        if( len(scriptnospaces) > 15 ):\n",
    "            lang = predecir(remove_html_tags(row[4]))\n",
    "            if lang == 'fr':\n",
    "                lang = 'es' \n",
    "            if lang == 'en' or lang == 'fr':\n",
    "                sentences = row[4].split(',')\n",
    "                for s in sentences:\n",
    "                    actuals = s.split('.')\n",
    "                    for s2 in actuals:\n",
    "                        actuals2 = s2.split('\\n')\n",
    "                        for s3 in actuals2:\n",
    "                            actuals3 = s3.split('<br>')\n",
    "                            for s4 in actuals3:\n",
    "                                if len(s4.split(' ')) > 3:\n",
    "                                    crs.append([\n",
    "                                        row[1].lower(),\n",
    "                                        remove_html_tags(s4.lower()),\n",
    "                                        lang\n",
    "                                       ])\n",
    "            b = \"Loading \" + str(i)\n",
    "            print (b, end=\"\\r\")\n",
    "            i+=1\n",
    "                \n",
    "crdf = DataFrame(crs,columns=['clientname','script', lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161352"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(crdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161351\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "619650"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "tokens = []\n",
    "for i in range(len(crdf)):\n",
    "    sentence = crdf.iloc[i]\n",
    "    if sentence['clientname'] in sentence['script']:\n",
    "        doc = nlp(sentence['script'])\n",
    "        for token in doc:\n",
    "            if token.pos_ != 'SPACE':\n",
    "                tag = 'O'\n",
    "                if token.text in sentence['clientname']:\n",
    "                    tag = 'C'\n",
    "                tokens.append([i+1,token.text,token.pos_,tag])\n",
    "    print (i, end=\"\\r\")\n",
    "len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda2/envs/tf/lib/python3.7/site-packages/ipykernel_launcher.py:8: FutureWarning: Indexing with multiple keys (implicitly converted to a tuple of keys) will be deprecated, use a list instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>text</th>\n",
       "      <th>post</th>\n",
       "      <th>tag</th>\n",
       "      <th>text_idx</th>\n",
       "      <th>tag_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[icon, night, club, the, newest, hottest, spot...</td>\n",
       "      <td>[PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...</td>\n",
       "      <td>[C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]</td>\n",
       "      <td>[18685, 1173, 8849, 6172, 13588, 12738, 10366,...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>[icon, night, club, your, new, favorite, club,...</td>\n",
       "      <td>[PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, NUM,...</td>\n",
       "      <td>[C, C, C, O, O, O, C, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[18685, 1173, 8849, 17266, 17119, 459, 8849, 4...</td>\n",
       "      <td>[0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>[ojos, locos, fight, night, 12/8]</td>\n",
       "      <td>[PROPN, PROPN, VERB, NOUN, NOUN]</td>\n",
       "      <td>[C, C, O, O, O]</td>\n",
       "      <td>[10301, 4199, 16516, 1173, 12393]</td>\n",
       "      <td>[0, 0, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>[get, to, ojos, locos, sports, cantina, and, w...</td>\n",
       "      <td>[AUX, ADP, PROPN, PROPN, PROPN, PROPN, CCONJ, ...</td>\n",
       "      <td>[O, O, C, C, O, O, O, O, O, O, O]</td>\n",
       "      <td>[6732, 6250, 10301, 4199, 3352, 17675, 11561, ...</td>\n",
       "      <td>[1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16</td>\n",
       "      <td>[ojos, locos, in, dallas, at, the, corner, of,...</td>\n",
       "      <td>[PROPN, PROPN, ADP, PROPN, ADP, DET, NOUN, ADP...</td>\n",
       "      <td>[C, C, O, O, O, O, O, O, O, O, O, O, O]</td>\n",
       "      <td>[10301, 4199, 18399, 17435, 16614, 6172, 15684...</td>\n",
       "      <td>[0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sentence                                               text  \\\n",
       "0         1  [icon, night, club, the, newest, hottest, spot...   \n",
       "1         8  [icon, night, club, your, new, favorite, club,...   \n",
       "2         9                  [ojos, locos, fight, night, 12/8]   \n",
       "3        10  [get, to, ojos, locos, sports, cantina, and, w...   \n",
       "4        16  [ojos, locos, in, dallas, at, the, corner, of,...   \n",
       "\n",
       "                                                post  \\\n",
       "0  [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...   \n",
       "1  [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, NUM,...   \n",
       "2                   [PROPN, PROPN, VERB, NOUN, NOUN]   \n",
       "3  [AUX, ADP, PROPN, PROPN, PROPN, PROPN, CCONJ, ...   \n",
       "4  [PROPN, PROPN, ADP, PROPN, ADP, DET, NOUN, ADP...   \n",
       "\n",
       "                                                 tag  \\\n",
       "0      [C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]   \n",
       "1  [C, C, C, O, O, O, C, O, O, O, O, O, O, O, O, ...   \n",
       "2                                    [C, C, O, O, O]   \n",
       "3                  [O, O, C, C, O, O, O, O, O, O, O]   \n",
       "4            [C, C, O, O, O, O, O, O, O, O, O, O, O]   \n",
       "\n",
       "                                            text_idx  \\\n",
       "0  [18685, 1173, 8849, 6172, 13588, 12738, 10366,...   \n",
       "1  [18685, 1173, 8849, 17266, 17119, 459, 8849, 4...   \n",
       "2                  [10301, 4199, 16516, 1173, 12393]   \n",
       "3  [6732, 6250, 10301, 4199, 3352, 17675, 11561, ...   \n",
       "4  [10301, 4199, 18399, 17435, 16614, 6172, 15684...   \n",
       "\n",
       "                                             tag_idx  \n",
       "0      [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]  \n",
       "1  [0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2                                    [0, 0, 1, 1, 1]  \n",
       "3                  [1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1]  \n",
       "4            [0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatokens = DataFrame(tokens,columns=['sentence','text', 'post','tag'])\n",
    "token2idx, idx2token = get_dict(datatokens, 'token')\n",
    "tag2idx, idx2tag = get_dict(datatokens, 'tag')\n",
    "datatokens['text_idx'] = datatokens['text'].map(token2idx)\n",
    "datatokens['tag_idx'] = datatokens['tag'].map(tag2idx)\n",
    "datatokensg = datatokens.groupby(\n",
    "['sentence'],as_index=False\n",
    ")['text', 'post', 'tag', 'text_idx', 'tag_idx'].agg(lambda x: list(x))\n",
    "datatokensg.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val(datatokensg, datatokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_dim:  19552 \n",
      "output_dim:  64 \n",
      "input_length:  163 \n",
      "n_tags:  2\n",
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_9 (Embedding)      (None, 163, 64)           1251328   \n",
      "_________________________________________________________________\n",
      "time_distributed_9 (TimeDist (None, 163, 2)            130       \n",
      "=================================================================\n",
      "Total params: 1,251,458\n",
      "Trainable params: 1,251,458\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "618/618 - 5s - loss: nan - accuracy: 0.0174 - val_loss: nan - val_accuracy: 0.0174\n",
      "Epoch 2/2\n",
      "618/618 - 6s - loss: nan - accuracy: 0.0174 - val_loss: nan - val_accuracy: 0.0174\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(datatokens['text'].to_list())))+1\n",
    "output_dim = 64\n",
    "input_length = max([len(s) for s in datatokensg['text_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "history = model.fit(train_tokens,np.array(train_tags),\n",
    "                  epochs=2,\n",
    "                  validation_split=0.20,\n",
    "                  shuffle=True,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_10 (Embedding)     (None, 163, 64)           1251328   \n",
      "_________________________________________________________________\n",
      "bidirectional_7 (Bidirection (None, 163, 128)          66048     \n",
      "_________________________________________________________________\n",
      "lstm_15 (LSTM)               (None, 163, 64)           49408     \n",
      "_________________________________________________________________\n",
      "time_distributed_10 (TimeDis (None, 163, 2)            130       \n",
      "=================================================================\n",
      "Total params: 1,366,914\n",
      "Trainable params: 1,366,914\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/2\n",
      "618/618 - 314s - loss: nan - accuracy: 0.0188 - val_loss: nan - val_accuracy: 0.0174\n",
      "Epoch 2/2\n",
      "618/618 - 312s - loss: nan - accuracy: 0.0174 - val_loss: nan - val_accuracy: 0.0174\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "history = model.fit(train_tokens,np.array(train_tags),\n",
    "                  epochs=2,\n",
    "                  validation_split=0.20,\n",
    "                  shuffle=True,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_11\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 163, 4)            78208     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 163, 8)            288       \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 163, 4)            208       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 163, 2)            10        \n",
      "=================================================================\n",
      "Total params: 78,714\n",
      "Trainable params: 78,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19771 samples, validate on 4943 samples\n",
      "Epoch 1/2\n",
      "19771/19771 - 203s - loss: nan - accuracy: 0.0178 - val_loss: nan - val_accuracy: 0.0174\n",
      "Epoch 2/2\n",
      "19771/19771 - 212s - loss: nan - accuracy: 0.0174 - val_loss: nan - val_accuracy: 0.0174\n"
     ]
    }
   ],
   "source": [
    "output_dim = 4\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "history = model.fit(train_tokens,np.array(train_tags),\n",
    "                  epochs=2,\n",
    "                  validation_split=0.20,\n",
    "                  shuffle=True,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3438,  9779, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550], dtype=int32)"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[56]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = []\n",
    "for k in range(len(datatokensg)):\n",
    "    lengths.append(len(datatokensg.iloc[k]['text']))\n",
    "lengths = np.array(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "163"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.0"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.median(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=36614, minmax=(1, 163), mean=16.923854263396514, variance=159.78400310508152, skewness=2.485677318324948, kurtosis=11.089954558493826)"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scipy.stats.describe(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10895"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths[lengths<10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4052"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths[lengths>30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36614"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Me di cuenta que mis datos estan muy desbalanceados ya que el largo de padding que tenia de 132 esta muy alejado a la mediana y media de la cantidad de palabras que yo quiero analizar por lo cual voy a hacer un trim y voy a agarrar unicamente la parte de las oraciones donde se mencione el cliente con un largo de 30 como maximo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32224"
      ]
     },
     "execution_count": 269,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdatatokensg = []\n",
    "for f in range(len(datatokensg)):\n",
    "    if len(datatokensg.iloc[f]['text']) < 30:\n",
    "        newdatatokensg.append(datatokensg.iloc[f])\n",
    "len(newdatatokensg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence                                                    1\n",
       "text        [icon, night, club, the, newest, hottest, spot...\n",
       "post        [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...\n",
       "tag             [C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]\n",
       "text_idx    [18685, 1173, 8849, 6172, 13588, 12738, 10366,...\n",
       "tag_idx         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdatatokensg[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdatatokensg = DataFrame(newdatatokensg,columns=['sentence','text', 'post','tag','text_idx','tag_idx'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pad_train_test_val_pad(data_group, data):\n",
    "\n",
    "    #get max token and tag length\n",
    "    n_token = len(list(set(data['text'].to_list())))\n",
    "    n_tag = len(list(set(data['tag'].to_list())))\n",
    "\n",
    "    #Pad tokens (X var)    \n",
    "    tokens = data_group['text_idx'].tolist()\n",
    "    maxlen = max([len(s) for s in tokens])\n",
    "    pad_tokens = pad_sequences(tokens, maxlen=maxlen, dtype='int32', padding='post', value= n_token - 1)\n",
    "\n",
    "    #Pad Tags (y var) and convert it into one hot encoding\n",
    "    tags = data_group['tag_idx'].tolist()\n",
    "    pad_tags = pad_sequences(tags, maxlen=maxlen, dtype='int32', padding='post', value= tag2idx[\"O\"])\n",
    "    n_tags = len(tag2idx)\n",
    "    pad_tags = [to_categorical(i, num_classes=n_tags) for i in pad_tags]\n",
    "    #Split train, test and validation set\n",
    "    tokens_, test_tokens, tags_, test_tags = train_test_split(pad_tokens, pad_tags, test_size=0.1, train_size=0.9, random_state=2020)\n",
    "    train_tokens, val_tokens, train_tags, val_tags = train_test_split(tokens_,tags_,test_size = 0.25,train_size =0.75, random_state=2020)\n",
    "    \n",
    "    return train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence                                                    1\n",
      "text        [icon, night, club, the, newest, hottest, spot...\n",
      "post        [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...\n",
      "tag             [C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]\n",
      "text_idx    [18685, 1173, 8849, 6172, 13588, 12738, 10366,...\n",
      "tag_idx         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "Name: 0, dtype: object\n",
      "[18685  1173  8849  6172 13588 12738 10366 18399  9730 19327  1173  6172\n",
      "  4482  3056 18939 19550 19550 19550 19550 19550 19550 19550 19550 19550\n",
      " 19550 19550 19550 19550 19550]\n",
      "[[1. 0.]\n",
      " [1. 0.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [1. 0.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]\n",
      " [0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "train_tokens, val_tokens, test_tokens, train_tags, val_tags, test_tags = get_pad_train_test_val_pad(newdatatokensg, datatokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21750, 29)"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sentence                                                    1\n",
       "text        [icon, night, club, the, newest, hottest, spot...\n",
       "post        [PROPN, NOUN, PROPN, DET, ADJ, ADJ, NOUN, ADP,...\n",
       "tag             [C, C, C, O, O, O, O, O, O, O, C, O, O, O, O]\n",
       "text_idx    [18685, 1173, 8849, 6172, 13588, 12738, 10366,...\n",
       "tag_idx         [0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
       "Name: 0, dtype: object"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datatokensg.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18150, 17205, 10691,  3200,  8202, 12884,  6459,  3552,  1138,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550, 19550,\n",
       "       19550, 19550], dtype=int32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tokens[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]], dtype=float32)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_11 (Embedding)     (None, 163, 4)            78208     \n",
      "_________________________________________________________________\n",
      "bidirectional_10 (Bidirectio (None, 163, 8)            288       \n",
      "_________________________________________________________________\n",
      "lstm_19 (LSTM)               (None, 163, 4)            208       \n",
      "_________________________________________________________________\n",
      "time_distributed_11 (TimeDis (None, 163, 2)            10        \n",
      "=================================================================\n",
      "Total params: 78,714\n",
      "Trainable params: 78,714\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 19771 samples, validate on 4943 samples\n",
      "Epoch 1/10\n",
      "19771/19771 - 203s - loss: nan - accuracy: 0.543 - val_loss: nan - val_accuracy: 0.574\n",
      "Epoch 2/10\n",
      "19771/19771 - 212s - loss: nan - accuracy: 0.548 - val_loss: nan - val_accuracy: 0.551\n",
      "Epoch 3/10\n",
      "19771/19771 - 230s - loss: nan - accuracy: 0.622 - val_loss: nan - val_accuracy: 0.618\n",
      "Epoch 4/10\n",
      "19771/19771 - 248s - loss: nan - accuracy: 0.677 - val_loss: nan - val_accuracy: 0.651\n",
      "Epoch 5/10\n",
      "19771/19771 - 235s - loss: nan - accuracy: 0.777 - val_loss: nan - val_accuracy: 0.723\n",
      "Epoch 6/10\n",
      "19771/19771 - 278s - loss: nan - accuracy: 0.822 - val_loss: nan - val_accuracy: 0.811\n",
      "Epoch 7/10\n",
      "19771/19771 - 250s - loss: nan - accuracy: 0.853 - val_loss: nan - val_accuracy: 0.832\n",
      "Epoch 8/10\n",
      "19771/19771 - 232s - loss: nan - accuracy: 0.867 - val_loss: nan - val_accuracy: 0.849\n",
      "Epoch 9/10\n",
      "19771/19771 - 245s - loss: nan - accuracy: 0.877 - val_loss: nan - val_accuracy: 0.866\n",
      "Epoch 10/10\n",
      "19771/19771 - 215s - loss: nan - accuracy: 0.881 - val_loss: nan - val_accuracy: 0.875\n"
     ]
    }
   ],
   "source": [
    "input_dim = len(list(set(datatokens['text'].to_list())))+1\n",
    "output_dim = 128\n",
    "input_length = max([len(s) for s in newdatatokensg['text_idx'].tolist()])\n",
    "n_tags = len(tag2idx)\n",
    "print('input_dim: ', input_dim, '\\noutput_dim: ', output_dim, '\\ninput_length: ', input_length, '\\nn_tags: ', n_tags)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=output_dim, input_length=input_length))\n",
    "model.add(Bidirectional(LSTM(units=output_dim, return_sequences=True, dropout=0.2, recurrent_dropout=0.2), merge_mode = 'concat'))\n",
    "model.add(LSTM(units=output_dim, return_sequences=True, dropout=0.5, recurrent_dropout=0.5))\n",
    "model.add(TimeDistributed(Dense(n_tags, activation=\"relu\")))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.summary()\n",
    "tensorboard = TensorBoard(log_dir=\"run\")\n",
    "history = model.fit(train_tokens,np.array(train_tags),\n",
    "                  epochs=10,\n",
    "                  validation_split=0.20,\n",
    "                  shuffle=True,\n",
    "                  verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "El darme cuenta de los paddings que estaba utilizando en mi embedding y de la comparacion de las palabras dependiendo del lenguaje fue crucial para poder entrenar el algoritmo de forma correca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
